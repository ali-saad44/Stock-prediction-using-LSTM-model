# -*- coding: utf-8 -*-
"""DS Intern T2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fiJ0ZOsS3X68iQTimPfnXK7dVRkr9x92

# **Get Data from Kaggle**
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("timoboz/tesla-stock-data-from-2010-to-2020")

print("Path to dataset files:", path)

import shutil

# Source (Colab cache)
src_path = "/root/.cache/kagglehub/datasets/timoboz/tesla-stock-data-from-2010-to-2020/versions/1"

# Destination (your Google Drive folder)
dst_path = "/content/drive/MyDrive/tesla_dataset"

# Copy entire folder to Drive
shutil.copytree(src_path, dst_path, dirs_exist_ok=True)

print("Dataset copied to:", dst_path)

"""# **Preprocssing**"""



import pandas as pd

# Load the CSV (usually named 'tesla.csv' or similar)
df = pd.read_csv("/content/drive/MyDrive/tesla_dataset/TSLA.csv")
print(df.head())

# Shape (rows, columns)
print("\nDataset Shape:", df.shape)

# Column details
print("\nDataset Info:")
print(df.info())

print("\nNull Values in Each Column:")
print(df.isnull().sum())

# Percentage of null values
print("\nPercentage of Null Values:")
print((df.isnull().sum() / len(df)) * 100)

print("\nStatistical Summary:")
print(df.describe())

"""# **Import libraries**"""

# Install dependencies (if not already installed)
!pip install tensorflow numpy pandas scikit-learn matplotlib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from math import sqrt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

"""# **Data Preparation and Training**"""

print("Dataset Shape:", df.shape)
print(df.head())

data = df[['Open', 'High', 'Low', 'Close', 'Volume']].values

# Scale features between 0 and 1
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

sequence_length = 60

X, y = [], []
for i in range(sequence_length, len(scaled_data)):
    X.append(scaled_data[i-sequence_length:i])
    y.append(scaled_data[i, 3])  # Predict 'Close' price

X, y = np.array(X), np.array(y)

print("X shape:", X.shape)
print("y shape:", y.shape)

# Train-test split (80-20)
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
    Dropout(0.2),  # helps prevent overfitting
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(25),
    Dense(1)  # output layer for predicted closing price
])

model.compile(optimizer='adam', loss='mean_squared_error')
model.summary()

history = model.fit(X_train, y_train, batch_size=32, epochs=30,
                    validation_data=(X_test, y_test), verbose=1)
y_pred = model.predict(X_test)

predictions = model.predict(X_test)

"""# **Evaluation**"""

# Inverse transform (bring back to original scale)
scaled_close = scaled_data[:, 3].reshape(-1, 1)
scaler_close = MinMaxScaler()
scaler_close.fit(scaled_close[:train_size])  # fit only on train
predicted_prices = scaler_close.inverse_transform(predictions)
real_prices = scaler_close.inverse_transform(y_test.reshape(-1, 1))

# RMSE
rmse = sqrt(mean_squared_error(real_prices, predicted_prices))
print("Test RMSE:", rmse)

plt.figure(figsize=(12,6))
plt.plot(real_prices, color='blue', label="Actual Prices")
plt.plot(predicted_prices, color='red', label="Predicted Prices")
plt.title("Tesla Stock Price Prediction")
plt.xlabel("Time")
plt.ylabel("Stock Price")
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title("Model Loss Over Epochs")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()



